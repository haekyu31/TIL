# TIL

## Algorithm
* DFS
* [226. Invert Binary Tree](https://github.com/haekyu31/LeetCode/commit/5c5298a8f32c1d3eb86e6a0ef56bd773f2224f6c)
* [501. Find Mode in Binary Search Tree](https://github.com/haekyu31/LeetCode/commit/95aaf183d1ba6e537643a8a59e084e7998d2f0d2)
* [98. Validate Binary Search Tree](https://github.com/haekyu31/LeetCode/commit/db903c615bf9670d62430838145a2ba5e1ae40c0)



## RNN
### Simple Neural Network
```python
# x1 + x2 - x3
Data_x = [
    [ 1.0, 0.8, 0.4 ],
    [ 1.0, 0.8, 0.7 ],
    [ 0.3, 0.1, 0.2 ]
]

Data_y = [
    1.4,
    1.1,
    0.2
]
nFeatures = 3
model = Sequential()
# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)
model.add(Dense(1, activation='linear', input_shape=( nFeatures, )))
model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=Adam(learning_rate=0.01), metrics=['mse'])
model.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 1)                 4         
                                                                 
=================================================================
Total params: 4
Trainable params: 4
Non-trainable params: 0
_________________________________________________________________
# input이 3이고 unit은 1이기 때문에 각 input에 weight와 상수를 합해서 parameter가 4이다.

model.weights  #weight를 구함
[<tf.Variable 'dense/kernel:0' shape=(3, 1) dtype=float32, numpy=
 array([[-0.27258205],
        [ 0.39902294],
        [ 0.15223753]], dtype=float32)>,
 <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]

model.predict(Data_x)  #모델로 예측
1/1 [==============================] - 0s 116ms/step
array([[ 0.10753132],
       [ 0.1532026 ],
       [-0.01142482]], dtype=float32)

model.fit(Data_x, Data_y, epochs=50)  #모델 fitting

model.weights  #학습한 모델로 weight를 구하기 
[<tf.Variable 'dense/kernel:0' shape=(3, 1) dtype=float32, numpy=
 array([[0.06337504],
        [0.7421818 ],
        [0.46736962]], dtype=float32)>,
 <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.3073927], dtype=float32)>]

model.predict(Data_x) #학습한 모델로 다시 예측
1/1 [==============================] - 0s 22ms/step
array([[1.151461 ],
       [1.2916719],
       [0.4940973]], dtype=float32)

Data_y  #실제 data y값  차이가 줄어든 것을 알 수 있다.
[1.4, 1.1, 0.2]
```
### Deep & Wide model, parameters
```python
model = Sequential()
# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)
model.add(Dense(30, activation='linear', input_shape=( nFeatures, )))
model.add(Dense(20, activation='linear'))
model.add(Dense(53, activation='linear'))
model.add(Dense(23, activation='linear'))
model.add(Dense(1, activation='linear'))
model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=Adam(learning_rate=0.01), metrics=['mse'])
# Deep, Wide한 모델

model.summary()
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_1 (Dense)             (None, 30)                120       
                                                                 
 dense_2 (Dense)             (None, 20)                620       
                                                                 
 dense_3 (Dense)             (None, 53)                1113      
                                                                 
 dense_4 (Dense)             (None, 23)                1242      
                                                                 
 dense_5 (Dense)             (None, 1)                 24        
                                                                 
=================================================================
Total params: 3,119
Trainable params: 3,119
Non-trainable params: 0
_________________________________________________________________
# 3개의 input 이 unit 30을 만나서 (3+1)* 30 = 120 parameter
# 30개의 input이 unit 20을 만나서 (30+1)* 20 = 620 parameter ...
# 마지막은 23개의 input을 하나의 output으로 내보내기 때문에 (23+1)*1 24개의 parameter
model.fit(Data_x, Data_y, epochs=50)
model.predict(Data_x)
1/1 [==============================] - 0s 55ms/step
array([[1.3521825 ],
       [1.0586601 ],
       [0.19386578]], dtype=float32)
Data_y
[1.4, 1.1, 0.2]
#정확도가 더 높아진 모습
```
### Recurrent Neural Network
```python
# Simple time series data (time = 6, nFeatures = 1)
# return_sequences = False
# ** Many to One **
Data_x = [
    [0.0, 0.0, 0.0, 0.5, 0.5, 1.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
]

Data_y = [ 
    [1],
    [0]
]
nFeatures = 1
TimeRange = 6

### Model architecture & parameters? return_sequences?
model = Sequential()
# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)
model.add(SimpleRNN(1, activation='linear', return_sequences = False, input_shape=( TimeRange, nFeatures,  )))
model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=Adam(learning_rate=0.01), metrics=['mse'])
model.summary()
Model: "sequential_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn (SimpleRNN)      (None, 1)                 3         
                                                                 
=================================================================
Total params: 3
Trainable params: 3
Non-trainable params: 0
_________________________________________________________________
# 1개의 unit, input은 6,1  parameter는 weight, 상수, time_data의 weight까지 3개
model.weights
[<tf.Variable 'simple_rnn/simple_rnn_cell/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-1.6714507]], dtype=float32)>,
 <tf.Variable 'simple_rnn/simple_rnn_cell/recurrent_kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-1.]], dtype=float32)>,
 <tf.Variable 'simple_rnn/simple_rnn_cell/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]
model.predict(Data_x)
1/1 [==============================] - 0s 125ms/step
array([[-1.6714507],
       [ 0.       ]], dtype=float32)

# 모델 학습하기 전 weight와 예측

model.fit(Data_x, Data_y, epochs=30)
#모델 학습
model.weights
[<tf.Variable 'simple_rnn/simple_rnn_cell/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-1.3808427]], dtype=float32)>,
 <tf.Variable 'simple_rnn/simple_rnn_cell/recurrent_kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-0.71393543]], dtype=float32)>,
 <tf.Variable 'simple_rnn/simple_rnn_cell/bias:0' shape=(1,) dtype=float32, numpy=array([0.30658665], dtype=float32)>]
model.predict(Data_x)
1/1 [==============================] - 0s 24ms/step
array([[-1.0846452 ],
       [ 0.15519163]], dtype=float32)
# 학습 후에 예측
# return_sequences = False 이기 때문에 time_series_data에 대한 결과를 저장하지 않고 가장 마지막만  output으로 내보낸다.
# return_sequences = False
# ** Many to One **
```
### 1. return_sequences??<br>
: MANY TO ONE vs MANY TO MANY
```python
# Simple time series data (time = 6, nFeatures = 1)
# return_sequences=True
# ** Many to Many **

Data_x = [
    [0.0, 0.0, 0.0, 0.5, 0.8, 1.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
]

Data_y = [ 
    [0.0, 0.0, 0.0, 1.0, 1.0, 1.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
]
nFeatures = 1
TimeRange = 6

model = Sequential()
# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)
model.add(SimpleRNN(1, activation='linear', return_sequences = True, input_shape=( TimeRange, nFeatures,  )))
model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=Adam(learning_rate=0.1), metrics=['mse'])
model.summary()
Model: "sequential_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn_2 (SimpleRNN)    (None, 6, 1)              3         
                                                                 
=================================================================
Total params: 3
Trainable params: 3
Non-trainable params: 0
_________________________________________________________________
# return_sequences = True 인 상태 이전의 time_serie_data를 가져와서 계산
# 각 시퀀스에서 출력할 수 있도록 한다.
model.weights
[<tf.Variable 'simple_rnn_2/simple_rnn_cell_2/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.50010383]], dtype=float32)>,
 <tf.Variable 'simple_rnn_2/simple_rnn_cell_2/recurrent_kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-1.]], dtype=float32)>,
 <tf.Variable 'simple_rnn_2/simple_rnn_cell_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]
 model.predict(Data_x)
1/1 [==============================] - 0s 114ms/step
array([[[0.        ],
        [0.        ],
        [0.        ],
        [0.25005192],
        [0.15003115],
        [0.35007268]],

       [[0.        ],
        [0.        ],
        [0.        ],
        [0.        ],
        [0.        ],
        [0.        ]]], dtype=float32)
# return_sequences = True 이기 때문에 TimeRange= 6만큼 각 시퀀스 결과를 보여준다
model.fit(Data_x, Data_y, epochs=30)
model.weights
[<tf.Variable 'simple_rnn_2/simple_rnn_cell_2/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[1.7205364]], dtype=float32)>,
 <tf.Variable 'simple_rnn_2/simple_rnn_cell_2/recurrent_kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-0.7233836]], dtype=float32)>,
 <tf.Variable 'simple_rnn_2/simple_rnn_cell_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.02333338], dtype=float32)>]
model.predict(Data_x)
1/1 [==============================] - 0s 24ms/step
array([[[0.02333338],
        [0.0064544 ],
        [0.01866438],
        [0.8701    ],
        [0.7703464 ],
        [1.1866138 ]],

       [[0.02333338],
        [0.0064544 ],
        [0.01866438],
        [0.00983188],
        [0.01622116],
        [0.01159926]]], dtype=float32)
# 결과에서 [0]은 점차 1에 가까운 값으로 결과가 예측 되는것을 볼 수 있고
# [1]은 결과가 점차 0에 가까워 지는 것을 볼 수 있다.
```

### 2. RNN Parameter 계산하는 것
```python
model = Sequential()
# RNN. 현재 RNN은 return_sequences=False (마지막 x time-step에 대해서만 output을 냄)
model.add(SimpleRNN(5, activation='linear', return_sequences = False, input_shape=( TimeRange, nFeatures,  )))
model.add(Dense(5, activation='linear'))
model.add(Dense(1, activation='linear'))
​
model.summary()
Model: "sequential_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn_3 (SimpleRNN)    (None, 5)                 35        
                                                                 
 dense_6 (Dense)             (None, 5)                 30        
                                                                 
 dense_7 (Dense)             (None, 1)                 6         
                                                                 
=================================================================
Total params: 71
Trainable params: 71
Non-trainable params: 0
_________________________________________________________________
# return_sequences = False 인경우 parameter 갯수
# input 6(time_series), unit 5 (6+1)*5 = 35
# input 5, unit 5 (5+1)*5 = 30
# input 5, unit 1 (5+1)* 1 = 6

model = Sequential()
# RNN. 현재 RNN은 return_sequences (모든 x time-step에 대하여 output을 냄)
model.add(SimpleRNN(5, activation='linear', return_sequences = True, input_shape=( TimeRange, nFeatures,  )))
model.add(SimpleRNN(5, activation='linear', return_sequences = False))
model.add(Dense(1, activation='linear'))
​
model.summary()
Model: "sequential_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn_4 (SimpleRNN)    (None, 6, 5)              35        
                                                                 
 simple_rnn_5 (SimpleRNN)    (None, 5)                 55        
                                                                 
 dense_8 (Dense)             (None, 1)                 6         
                                                                 
=================================================================
Total params: 96
Trainable params: 96
Non-trainable params: 0
_________________________________________________________________
# return_sequences = True, 와 False를 함께 한 넣은 경우
# input 6 (time_seires), unit 5 (6+1)*5 = 35
# input 5 (time_series), return_sequences 5, unit 5 (5+5+1)*5 = 55
# input 5 unit 1 (6)*1
```
### 3. TimeDistributed (time 마다 back-progation 수행)
안쓰면 Many to One 처럼 계산하게 된다.
```python
Data_x = [
    [0.0, 0.0, 0.0, 0.5, 0.8, 1.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
]

Data_y = [ 
    [0.0, 0.0, 0.0, 1.0, 1.0, 1.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
]
model = Sequential()
# RNN. 현재 RNN은 return_sequences=False (마지막 x time-step에 대해서만 output을 냄)
model.add(SimpleRNN(5, activation='linear', return_sequences = True, input_shape=( TimeRange, nFeatures,  )))
model.add(Dense(1, activation='linear'))

model.summary()

Model: "sequential_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn_6 (SimpleRNN)    (None, 6, 5)              35        
                                                                 
 dense_9 (Dense)             (None, 6, 1)              6         
                                                                 
=================================================================
Total params: 41
Trainable params: 41
Non-trainable params: 0
_________________________________________________________________
model.predict(Data_x)
array([[[ 0.        ],
        [ 0.        ],
        [ 0.        ],
        [-0.17737417],
        [ 0.42635086],
        [ 0.77417904]],

       [[ 0.        ],
        [ 0.        ],
        [ 0.        ],
        [ 0.        ],
        [ 0.        ],
        [ 0.        ]]], dtype=float32)
# time_distributed 6개의 time_series를 다 보여준다

# **Final output layer의 unit 수가 1임에도 불구하고 Multiple output**
model = Sequential()
# RNN. 현재 RNN은 return_sequences=False (마지막 x time-step에 대해서만 output을 냄)
model.add(SimpleRNN(5, activation='linear', return_sequences = True, input_shape=( TimeRange, nFeatures,  )))
model.add(TimeDistributed(Dense(1, activation='linear')))

model.summary()
Model: "sequential_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn_7 (SimpleRNN)    (None, 6, 5)              35        
                                                                 
 time_distributed (TimeDistr  (None, 6, 1)             6         
 ibuted)                                                         
                                                                 
=================================================================
Total params: 41
Trainable params: 41
Non-trainable params: 0
_________________________________________________________________
model.predict(Data_x)
array([[[0.        ],
        [0.        ],
        [0.        ],
        [0.11392197],
        [0.15558073],
        [0.20875749]],

       [[0.        ],
        [0.        ],
        [0.        ],
        [0.        ],
        [0.        ],
        [0.        ]]], dtype=float32)

model = Sequential()
# RNN. 현재 RNN은 return_sequences=False (마지막 x time-step에 대해서만 output을 냄)
model.add(SimpleRNN(5, activation='linear', return_sequences = True, input_shape=( TimeRange, nFeatures,  )))
model.add(Dense(1, activation='linear'))
model.summary()


model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=Adam(learning_rate=0.01), metrics=['mse'])
model.fit(Data_x, Data_y, epochs=10)
```

### 4. Convolution
- Conv1D의 개념을 익힌다.
- Pooling1D
- GlobalAveragePooling2D()
- GlobalAveragePooling1D()
<br>https://www.geeksforgeeks.org/keras-conv2d-class/

```python
# 이미지 읽기
Image = PIL.Image.open('image.jpg')
np.array(Image).shape
np.array(Image)[:5, :5, 1]
array([[140, 140, 140, 140, 142],
       [139, 140, 140, 142, 143],
       [140, 141, 142, 142, 141],
       [142, 142, 143, 143, 143],
       [140, 141, 141, 143, 143]], dtype=uint8)

np.array(Image.resize([224, 224])).shape
(224, 224, 3)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(1, (3, 3), activation='linear', input_shape=(56, 56, 3,)))
model = Model(inputs=model.input, outputs=model.output)

Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_input (InputLayer)   [(None, 56, 56, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 54, 54, 1)         28        
                                                                 
=================================================================
Total params: 28
Trainable params: 28
Non-trainable params: 0
_________________________________________________________________
# filter 1, (3,3) output (56-2, 56-2, 1)
# (56(input_image_size) - 3(siez_kernels) + 2*0(padding_size))/1(stride) +1(상수)
```
![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a5325dc0b1b8695f19ec8fe3485d0da19040c622)
```python

# Pooling이 있는 모델의 경우
model = models.Sequential()
model.add(tf.keras.layers.Conv2D(5, (3, 3), activation='relu', padding='same', input_shape=(56, 56, 3,)))
model.add(tf.keras.layers.Conv2D(5, (3, 3), activation='relu', padding='same', input_shape=(56, 56, 3,)))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
model = Model(inputs=model.input, outputs=model.output)


model.summary()
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_1_input (InputLayer)  [(None, 56, 56, 3)]      0         
                                                                 
 conv2d_1 (Conv2D)           (None, 56, 56, 5)         140       
                                                                 
 conv2d_2 (Conv2D)           (None, 56, 56, 5)         230       
                                                                 
 max_pooling2d (MaxPooling2D  (None, 28, 28, 5)        0         
 )                                                               
                                                                 
 flatten (Flatten)           (None, 3920)              0         
                                                                 
 dense_13 (Dense)            (None, 1)                 3921      
                                                                 
=================================================================
Total params: 4,291
Trainable params: 4,291
Non-trainable params: 0
_________________________________________________________________
#
#Kernel: In image processing kernel is a convolution matrix or masks which can be used for blurring, sharpening, embossing, edge detection, and more by doing a convolution between a kernel and an image.
# Flatten 벡터 형태로 바꾸는 방식

```
```python
# 1D Convolution
Data_x = [
    [0.0, 0.0, 0.0, 0.5, 0.8, 1.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
]

Data_y = [ 
    [0.0, 0.0, 0.0, 1.0, 1.0, 1.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
]
TimeRange = 6
nFeatures = 1
num_filters = 1
kernel_size = 3

model = Sequential()
model.add(tf.keras.layers.Conv1D(1, 3, padding='valid', activation='relu', input_shape=(6, 1,) ))

Model: "sequential_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 4, 1)              4         
                                                                 
=================================================================
Total params: 4
Trainable params: 4
Non-trainable params: 0
_________________________________________________________________

model.weights
[<tf.Variable 'conv1d/kernel:0' shape=(3, 1, 1) dtype=float32, numpy=
 array([[[0.9049604 ]],
 
        [[0.0942049 ]],
 
        [[0.10145855]]], dtype=float32)>,
 <tf.Variable 'conv1d/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]
# kernel_size =3 , nFeatures = 1

model.predict(Data_x)
1/1 [==============================] - 0s 72ms/step
array([[[0.        ],
        [0.05072927],
        [0.12826928],
        [0.6293027 ]],

       [[0.        ],
        [0.        ],
        [0.        ],
        [0.        ]]], dtype=float32)


Data_x = [
    [[0, 0],[0, 0],[0, 0],[.5,0],[0,.5],[1, 1]],
    [[0, 0],[0, 0],[0, 0],[0, 0],[0, 0],[0, 0]]
]

Data_y = [ 
    [0.0, 0.0, 0.0, 1.0, 1.0, 1.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
]
TimeRange = 6
nFeatures = 2
num_filters = 1
kernel_size = 3
model = Sequential()
model.add(tf.keras.layers.Conv1D(1, 3, padding='valid', activation='relu', input_shape=(6, 2,)))

Model: "sequential_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d_1 (Conv1D)           (None, 4, 1)              7         
                                                                 
=================================================================
Total params: 7
Trainable params: 7
Non-trainable params: 0
_________________________________________________________________
model.weights
[<tf.Variable 'conv1d_1/kernel:0' shape=(3, 2, 1) dtype=float32, numpy=
 array([[[ 0.39629316],
         [-0.40588757]],
 
        [[-0.05358392],
         [ 0.12586159]],
 
        [[-0.5576509 ],
         [ 0.6925887 ]]], dtype=float32)>,
 <tf.Variable 'conv1d_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]
# kernel_size =3 , nFeatures = 2

array([[[0.        ],
        [0.        ],
        [0.31950238],
        [0.39601514]],

       [[0.        ],
        [0.        ],
        [0.        ],
        [0.        ]]], dtype=float32)

```
https://learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/      
## LSTM
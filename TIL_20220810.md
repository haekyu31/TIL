# TIL
0. layer
- image의 크기 (150, 150, 3)의 형태
- parameter 계산하는법
    > 매개변수, 특정한 성질을 나타내는 변수를 말한다.<br>
    > con2d 에 input (150,150,3) 형태 width 150 height 105 RGB 3개의 형태를 계산할 때 (3,3)의 이미지 필터를 이용하면 1개의 필터는 3 X 3의 9개 파라미터를 갖고있다. 각각 RGB에 해당하니까  X3이 되고 32개의 필터(유닛)을 사용했으므로 ((3x3) x3 +1) x32 = 896  두번째 conv2d 입력이 (72,72,32) (3,3)핕터가 64개 일때 ((3x3)X32+1)x64 = 18496개
---
1. Convolution layer란
    - 위키백과에서는 합성곱(合成-), 또는 콘벌루션(convolution)은 하나의 함수와 또 다른 함수를 반전 이동한 값을 곱한 다음, 구간에 대해 적분하여 새로운 함수를 구하는 수학 연산자이다. 
    - 합성곱 신경망(콘볼루션 신경망, Convolutional neural network, CNN)은 시각적 영상을 분석하는 데 사용되는 다층의 피드-포워드적인 인공신경망의 한 종류이다. 필터링 기법을 인공신경망에 적용하여 이미지를 효과적으로 처리할 수 있는 심층 신경망 기법으로 행렬로 표현된 필터의 각 요소가 데이터 처리에 적합하도록 자동으로 학습되는 과정을 통해 이미지를 분류하는 기법이다.
    ![Convolution](http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif)   
    출처 : http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif
   - filter 는 image의 특징을 찾기위한 공용 파라미터 kernel 이라고도 한다.(3,3),(4,4)의 형태로 정의된다
   - Unit 에 따른 output matrix(image)의 수는 하나의 필터마다 하나의 Activation map을 생성한다. 
   - Convolution layer의 parameter 수 계산하는법
---
2. Pooling layer
    - 출력된 Activation map의 크기를 줄이려고 할때 사용된다  Max Pooling, Average Pooling, Min Pooling이 있다.
    - image를 filter로 적용할때는 1칸씩 가로 세로 방향으로 움직이게 된다. Stride 1을 적용하게 되면 filter 의 크기만큼 움직이게 된다. 그리고 filter크기를 가진 Activation map이 생성된다.
    - pool_size zero_Pooling은 filter만큼의 이미지로 줄이는게 아닌 0의 값을 테두리로 둘러싸는 방식으로  padding을 실행해도 이미지의 크기가 줄어들지 않도록 만들수 있다. 
    
---
3. Fine-tunning<br>
    기존 학습되어있는 모델을 기반으로 아키텍쳐를 새로운 목적에 맞게 변형하고 이미 학습된 모델의 가중치를 미세하게 조정하여 학습시키는 방법
- fit 함수에서 validation<br>
    validation은 testset은 아니지만 train으로 학습한 모델의 정확도를 조정하기 위해 검정하는 set라고 할 수 있다. 모델의 예측을 평가하는데 사용된다.
- compile 함수에서 loss<br>
    ```python
    model.compile(loss='mse', optimizer='adam', metrics='acc')
    ```
    출처: https://ebbnflow.tistory.com/122?category=895675 [삶은 확률의 구름:티스토리]<br>
    loss 훈련손실, acc 훈련정확, val_loss 검증손실, val_acc 검증정확
    
- Binary cross-entropy Mean squarred error
- metric 함수에서 acc, mse, mae
---
1. Fully connected later를 만드는 방법
- flatten(), globalAveragerPooling()<br>
    차원이 너무나도 커진 map의 크기를 1차원의 vector로 바꾸기 위해서 사용한다.
    
    ![flatten](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png)<br>
    What happens after the flattening step is that you end up with a long vector of input data that you then pass through the artificial neural network to have it processed further. <br>
    출처 : https://www.superdatascience.com/blogs/convolutional-neural-networks-cnn-step-3-flattening
---
1. Call-back 함수<br>
        최적의 모델이 나온 loss, acc를 저장하여 log로 확인할수 있도록 만든 함수
- CP : Model Check-point 최적의 모델 저장하여 나중에 불러오기
- LR : Reduce Learning Rate : Learning rate 를 처음에 크게, 나중에 작게
- TB : TensorBoard 학습 진행을 시각화 (loss, accuracy의 변화)
# TIL

1. 단어 -> 정수인코딩 (Tekenizer)
```python
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
X_train_encoded = tokenizer.texts_to_sequences(X_train)
print(X_train_encoded[:5])
```

2. -> 앞에 0으로 채워준다 (padding)
-> 모든데이터가 같은 길이로 다음과 같이 만들어 짐
[0,0,0,0,. ......... , 10, 20, 32, 22]
 #모든 데이터가 똑같은 길이로 정형 데이터화 됨.
```python
# 패딩 : 긴 텍스트는 자르고, 짧은 텍스트는 패딩을 더해서 늘리고.
max_len = 189
X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)
print("훈련 데이터의 크기(shape):", X_train_padded.shape)
```

3. Embedding -> 글자 하나가 하나의 숫자
거치게 되면 -> embedding_dim의 수 만큼 dimension이 증가함
(예를 들면, 한 문장의 최대 길이가 189가정)
7개의 글자 -> 0을 포함하여 189개의 글자로 됨 (182개=0, 7개=글자)
embedding (64)을 거리면
189 * 64 개의 matrix로 바뀜
-> 64개의 feature가지고, 189개의 time을 가지는 time-series 데이터
-> RNN (LSTM, GRU..) / 1D conv로 학습할 수 있음
```python
from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential

embedding_dim = 64
hidden_units = 64

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=189))
model.add(SimpleRNN(hidden_units))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train_padded, y_train, epochs=13, batch_size=128, validation_split=0.2)
```

4. Sequence to Sequence 번역기
   - 태그를 달아서 동작을 제어한다.
    ```python
    # 태그 단어
    PAD = "<PADDING>"   # 패딩
    STA = "<START>"     # 시작
    END = "<END>"       # 끝
    OOV = "<OOV>"       # 없는 단어(Out of Vocabulary)

    # 형태소 분석
    # 단어와 인덱스 딕셔너리 단어->인덱스, 인덱스->단어 두가지 형태 사용
    # 문장을 인덱스로 변환
    def convert_text_to_index(sentences, vocabulary, type): 
        
        sentences_index = []
        
        # 모든 문장에 대해서 반복
        for sentence in sentences:
            sentence_index = []
            
            # 디코더 입력일 경우 맨 앞에 START 태그 추가
            if type == DECODER_INPUT:
                sentence_index.extend([vocabulary[STA]])
            
            # 문장의 단어들을 띄어쓰기로 분리
            for word in sentence.split():
                if vocabulary.get(word) is not None:
                    # 사전에 있는 단어면 해당 인덱스를 추가
                    sentence_index.extend([vocabulary[word]])
                else:
                    # 사전에 없는 단어면 OOV 인덱스를 추가
                    sentence_index.extend([vocabulary[OOV]])

            # 최대 길이 검사
            if type == DECODER_TARGET:
                # 디코더 목표일 경우 맨 뒤에 END 태그 추가
                if len(sentence_index) >= max_sequences:
                    sentence_index = sentence_index[:max_sequences-1] + [vocabulary[END]]
                else:
                    sentence_index += [vocabulary[END]]
            else:
                if len(sentence_index) > max_sequences:
                    sentence_index = sentence_index[:max_sequences]
                
            # 최대 길이에 없는 공간은 패딩 인덱스로 채움
            sentence_index += (max_sequences - len(sentence_index)) * [vocabulary[PAD]]
            
            # 문장의 인덱스 배열을 추가
            sentences_index.append(sentence_index)

        return np.asarray(sentences_index)

    # 모델생성
    #--------------------------------------------
    # 훈련 모델 인코더 정의
    #--------------------------------------------

    # 입력 문장의 인덱스 시퀀스를 입력으로 받음
    encoder_inputs = layers.Input(shape=(None,))

    # 임베딩 레이어
    encoder_outputs = layers.Embedding(len(words), embedding_dim)(encoder_inputs)

    # return_state가 True면 상태값 리턴
    # LSTM은 state_h(hidden state)와 state_c(cell state) 2개의 상태 존재
    encoder_outputs, state_h, state_c = layers.LSTM(lstm_hidden_dim,
                                                    dropout=0.1,
                                                    recurrent_dropout=0.5,
                                                    return_state=True)(encoder_outputs)

    # 히든 상태와 셀 상태를 하나로 묶음
    # Decoder의 initial state에 넣어주기 위함
    # 즉, input sentence의 모든 정보를 통해 Decoding 하기 위함
    encoder_states = [state_h, state_c]

    #--------------------------------------------
    # 훈련 모델 디코더 정의
    #--------------------------------------------

    # 목표 문장의 인덱스 시퀀스를 입력으로 받음
    decoder_inputs = layers.Input(shape=(None,))

    # 임베딩 레이어
    decoder_embedding = layers.Embedding(len(words), embedding_dim)
    decoder_outputs = decoder_embedding(decoder_inputs)

    # 인코더와 달리 return_sequences를 True로 설정하여 모든 타임 스텝 출력값 리턴
    # 모든 타임 스텝의 출력값들을 다음 레이어의 Dense()로 처리하기 위함
    decoder_lstm = layers.LSTM(lstm_hidden_dim,
                            dropout=0.1,
                            recurrent_dropout=0.5,
                            return_state=True,
                            return_sequences=True)

    # initial_state를 인코더의 상태로 초기화
    decoder_outputs, _, _ = decoder_lstm(decoder_outputs,
                                        initial_state=encoder_states)

    # 단어의 개수만큼 노드의 개수를 설정하여 원핫 형식으로 각 단어 인덱스를 출력
    decoder_dense = layers.Dense(len(words), activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)

    #--------------------------------------------
    # 훈련 모델 정의
    #--------------------------------------------

    # 입력과 출력으로 함수형 API 모델 생성
    model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)

    # 학습 방법 설정
    model.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['acc'])    

    #--------------------------------------------
    #  예측 모델 인코더 정의
    #--------------------------------------------

    # 훈련 모델의 인코더 상태를 사용하여 예측 모델 인코더 설정
    encoder_model = models.Model(encoder_inputs, encoder_states)

    #--------------------------------------------
    # 예측 모델 디코더 정의
    #--------------------------------------------

    # 예측시에는 훈련시와 달리 타임 스텝을 한 단계씩 수행
    # 매번 이전 디코더 상태를 입력으로 받아서 새로 설정
    decoder_state_input_h = layers.Input(shape=(lstm_hidden_dim,))
    decoder_state_input_c = layers.Input(shape=(lstm_hidden_dim,))
    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]    

    # 임베딩 레이어
    decoder_outputs = decoder_embedding(decoder_inputs)

    # LSTM 레이어
    decoder_outputs, state_h, state_c = decoder_lstm(decoder_outputs,
                                                    initial_state=decoder_states_inputs)

    # 히든 상태와 셀 상태를 하나로 묶음
    decoder_states = [state_h, state_c]

    # Dense 레이어를 통해 원핫 형식으로 각 단어 인덱스를 출력
    decoder_outputs = decoder_dense(decoder_outputs)

    # 예측 모델 디코더 설정
    decoder_model = models.Model([decoder_inputs] + decoder_states_inputs,
                        [decoder_outputs] + decoder_states)
    # 인덱스를 문장으로 변환
    def convert_index_to_text(indexs, vocabulary): 
        
        sentence = ''
        
        # 모든 문장에 대해서 반복
        for index in indexs:
            if index == END_INDEX:
                # 종료 인덱스면 중지
                break;
            elif vocabulary.get(index) is not None:
                # 사전에 있는 인덱스면 해당 단어를 추가
                sentence += vocabulary[index]
            else:
                # 사전에 없는 인덱스면 OOV 단어를 추가
                sentence += vocabulary[OOV_INDEX]
                
            # 빈칸 추가
            sentence += ' '

        return sentence

        # 에폭 반복
    for epoch in range(20):
        print('Total Epoch :', epoch + 1)

        # 훈련 시작
        history = model.fit([x_encoder, x_decoder],
                            y_decoder,
                            epochs=100,
                            batch_size=64,
                            verbose=0)
        
        # 정확도와 손실 출력
        print('accuracy :', history.history['acc'][-1])
        print('loss :', history.history['loss'][-1])
        
        # 문장 예측 테스트
        # (3 박 4일 놀러 가고 싶다) -> (여행 은 언제나 좋죠)
        input_encoder = x_encoder[2].reshape(1, x_encoder[2].shape[0])
        input_decoder = x_decoder[2].reshape(1, x_decoder[2].shape[0])
        results = model.predict([input_encoder, input_decoder])
        
        # 결과의 원핫인코딩 형식을 인덱스로 변환
        # 1축을 기준으로 가장 높은 값의 위치를 구함
        indexs = np.argmax(results[0], 1) 
        
        # 인덱스를 문장으로 변환
        sentence = convert_index_to_text(indexs, index_to_word)
        print(sentence)
        print()

    # 텍스트 생성
    def generate_text(input_seq):
        
        # 입력을 인코더에 넣어 마지막 상태 구함
        states = encoder_model.predict(input_seq)

        # 목표 시퀀스 초기화
        target_seq = np.zeros((1, 1))
        
        # 목표 시퀀스의 첫 번째에 <START> 태그 추가
        target_seq[0, 0] = STA_INDEX
        
        # 인덱스 초기화
        indexs = []
        
        # 디코더 타임 스텝 반복
        while 1:
            # 디코더로 현재 타임 스텝 출력 구함
            # 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화
            decoder_outputs, state_h, state_c = decoder_model.predict(
                                                    [target_seq] + states)

            # 결과의 원핫인코딩 형식을 인덱스로 변환
            index = np.argmax(decoder_outputs[0, 0, :])
            indexs.append(index)
            
            # 종료 검사
            if index == END_INDEX or len(indexs) >= max_sequences:
                break

            # 목표 시퀀스를 바로 이전의 출력으로 설정
            target_seq = np.zeros((1, 1))
            target_seq[0, 0] = index
            
            # 디코더의 이전 상태를 다음 디코더 예측에 사용
            states = [state_h, state_c]

        # 인덱스를 문장으로 변환
        sentence = convert_index_to_text(indexs, index_to_word)
            
        return sentence

    # 문장을 인덱스로 변환
    input_seq = make_predict_input('휴강이 좋아요')
    input_seq

    # 예측 모델로 텍스트 생성
    sentence = generate_text(input_seq)
    sentence

    # 문장을 인덱스로 변환
    input_seq = make_predict_input('데이터 강의 너무 좋아요')
    input_seq


    # 예측 모델로 텍스트 생성
    sentence = generate_text(input_seq)
    sentence
    ```

# TIL

## Algorithm
1. DFS Node 끝까지 확인하는 방식
2. BFS 하나의 Node에서 연결된 Node를 모두 방문한 뒤에 다음 연결된 Node를 탐색하는 방식

## LeetCode
1. [17. Letter Combinations of a Phone Number](https://github.com/haekyu31/LeetCode/commit/b6ee21f699f686bdbe76fbfd5f0f900fbac423ec)
2. [429. N-ary Tree Level Order Traversal](https://github.com/haekyu31/LeetCode/commit/1332e020bdfd5391d6b058dae6fc7410dba2bfda)
3. [938. Range Sum of BST](https://github.com/haekyu31/LeetCode/commit/9d5a31b2ef3cd5e59b1c75126829fd222d60d48f)
4. [404. Sum of Left Leaves](https://github.com/haekyu31/LeetCode/commit/eb6a923934bfa0bab7178cbfae27b9ba6bfc4f95)


## DataScience
1. RCNN
    - image를 네모 칸으로 나누어 탐색한다.
    ```python
    def extract_candidates(img):
        img_lbl, regions = selectivesearch.selective_search(img, scale=100, min_size=100)
        img_area = np.prod(img.shape[:2])
        candidates = []
        for r in regions:
            # excluding same rectangle (with different segments)
            if r['rect'] in candidates:
                continue
            # excluding regions smaller than 2000 pixels
            if r['size'] < (0.01*img_area):
                continue
            if r['size'] > (0.99*img_area):
                continue
            # distorted rects
            x, y, w, h = r['rect']
        #     if w / h > 1.2 or h / w > 1.2:
        #         continue
            candidates.append(list(r['rect']))
        return candidates
    #filename = jpegs + single_object_images[ix]
    url = 'https://image.dongascience.com/Photo/2020/03/15856430426741.jpg'
    img = PIL.Image.open(requests.get(url, stream=True).raw)
    img = tf.keras.preprocessing.image.img_to_array(img)

    img = cv2.resize(img,(224,224))
    img_area = img.shape[0]*img.shape[1]
    candidates = extract_candidates(img)
    plt.imshow(img/255)

    for x, y, w, h in candidates:
        img[y:(y+h), x, 1] = 1
        img[y:(y+h), (x+w), 1] = 1
        img[y, x:(x+w), 1] = 1
        img[y+h, x:(x+w), 1] = 1

    img = cv2.resize(img,(224,224))
    img_area = img.shape[0]*img.shape[1]
    candidates = extract_candidates(img)
    plt.imshow(img/255)
    ```
    - XML format으로 label 준다. (Class name, xmin, ymin, xmax, ymax)
    ```python
    training_data_size = N = 17

    final_cls = []
    final_delta = []
    iou_list = []
    imgs = []

    for ix, xml in enumerate(XMLs):
        
        if(ix%50==0):
            print(ix)
                
        print('Extracted data from {} xmls...'.format(ix), end='\r')
        xml_file = annotations + xml
        fname = xml.split('.')[0]
    
        with open(xml_file, "rb") as f:    # notice the "rb" mode
            xml = xmltodict.parse(f, xml_attribs=True)
            l = []
            
            if isinstance(xml["annotation"]["object"], list):
                #'let us ignore cases with multiple objects...'
                continue

            #'extracting coordinates...'
            bndbox = xml['annotation']['object']['bndbox']
            
            for key in bndbox:
                bndbox[key] = float(bndbox[key])
            x1, x2, y1, y2 = [bndbox[key] for key in ['xmin', 'xmax', 'ymin', 'ymax']]
            
            img_size = xml['annotation']['size']
            
            for key in img_size:
                img_size[key] = float(img_size[key])
            w, h = img_size['width'], img_size['height']

            #'converting pixel values from bndbox to fractions...'
            x1 /= w; x2 /= w; y1 /= h; y2 /= h
            label = xml['annotation']['object']['name']
            
            y = [x1, y1, x2-x1, y2-y1, label] # i.e., top-left x & y, width and height
            
            filename = jpegs+fname+'.jpg' # Path to the image files to be provided here
            
            img = cv2.resize(cv2.imread(filename), (224,224)) # since VGG's input shape is 224x224
            candidates = extract_candidates(img)
            
            for jx, candidate in enumerate(candidates):

                current_y2 = [int(i*224) for i in [x1,y1,x2,y2]] # [int(x1*224), int(y1*224), int(x2*224), int(y2*224)]

                iou = extract_iou2(candidate, current_y2, (224, 224))

                candidate_region_coordinates = c_x1, c_y1, c_w, c_h = np.array(candidate)/224

                dx = c_x1 - x1 
                dy = c_y1 - y1 
                dw = c_w - (x2-x1)
                dh = c_h - (y2-y1)

                final_delta.append([dx,dy,dw,dh])         

                if(iou>0.3):                    
                    final_cls.append(label)
                else:
                    final_cls.append('background')

                #"We'll predict our candidate crop using VGG"
                l = int(c_x1 * 224)
                r = int((c_x1 + c_w) * 224)
                t = int(c_y1 * 224)
                b = int((c_y1 + c_h) * 224)

                img2 = img[t:b,l:r,:3]
                img3 = cv2.resize(img2,(224,224))/255
                img4 = vgg16_model.predict(img3.reshape(1,224,224,3))
                imgs.append(img4[0])
                
                iou_list.append(iou)
    targets = pd.DataFrame(final_cls, columns=['label'])
    print(targets.head())
    labels = pd.get_dummies(targets['label']).columns
    print(labels[:5])
    y_train = pd.get_dummies(targets['label']).values.astype(float)
    print(y_train[:5])
    ```
    -  Selective search (SR)
    ```python
    from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D,UpSampling2D, Dropout, Cropping2D,  concatenate
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler
    from tensorflow.keras import backend as K
    from tensorflow.keras.models import Model

    model = Sequential()
    model.add(Flatten(input_shape=((7,7,512))))
    model.add(Dropout(0.5))
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(y_train.shape[1],activation='softmax'))
    model.summary()

    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])
    ```

    -> 임의의 네모칸을 찾아서 object인것 같은 것을 추천해준다.
    (대부분은 background겠지만, iou > 0.3 이상인 것은 label로 학습)
    -  Classification model의 성능
    ```python
    def test_predictions(filename):
        img = cv2.resize(cv2.imread(filename), (224,224))
        candidates = extract_candidates(img)

        _, ax = plt.subplots(1, 2)

        ax[0].imshow(img)
        ax[0].grid('off')
        ax[0].set_title(filename.split('/')[-1])


        pred = []
        pred_class = []

        for ix, candidate in enumerate(candidates):

            l, t, w, h = np.array(candidate).astype(int)

            img2 = img[t:t+h,l:l+w,:3]
            img3 = cv2.resize(img2,(224,224))/255
            img4 = vgg16_model.predict(img3.reshape(1,224,224,3))
            final_pred = model.predict(img4/x_train.max())
            pred.append(np.max(final_pred))
            pred_class.append(np.argmax(final_pred))

        pred = np.array(pred)
        pred_class = np.array(pred_class)

        pred2 = pred[pred_class!=1]
        pred_class2 = pred_class[pred_class!=1]

        candidates2 = np.array(candidates)[pred_class!=1]

        x, y, w, h = candidates2[np.argmax(pred2)]

        ax[1].set_title(labels[pred_class2[np.argmax(pred2)]])
        ax[1].imshow(img)
        ax[1].grid('off')
        rect = mpatches.Rectangle((x, y), w, h, fill=False, edgecolor='blue', linewidth=3)
        ax[1].add_patch(rect)
    ```
    2번 (SR)에서 추천 된 작은 이미지들에 대한 prediction 및 softmax
# TIL

## Algorithm
### LeetCode
1. [14. Longest Common Prefix](https://github.com/haekyu31/LeetCode/blob/master/14-longest-common-prefix/14-longest-common-prefix.py)
2. [48. Rotate Image](https://github.com/haekyu31/LeetCode/blob/master/48-rotate-image/48-rotate-image.py)
3. [236. Lowest Common Ancestor of a Binary Tree](https://github.com/haekyu31/LeetCode/blob/master/236-lowest-common-ancestor-of-a-binary-tree/236-lowest-common-ancestor-of-a-binary-tree.py)


## DataScience
1. LSTM 으로 텍스트 분류
```python
# X_train에 들어있는 숫자들이 각자 어떤 단어들을 나타내고 있는지 확인해보겠습니다. 
# reuters.get_word_index는 각 단어와 그 단어에 부여된 인덱스를 리턴합니다.
word_to_index = reuters.get_word_index()

print(len(word_to_index))
print(word_to_index)

vocab_size = 1000
max_len = 100

(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=vocab_size, test_split=0.2)

# 패딩
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

# 훈련용, 테스트용 뉴스 기사 데이터의 레이블에 원-핫 인코딩
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)


%%time
# 하이퍼파라미터인 임베딩 벡터의 차원은 128, 은닉 상태의 크기는 128입니다.
embedding_dim = 128
hidden_units = 128
num_classes = 46

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, y_test))

loaded_model = load_model('best_model.h5')
print("\n 테스트 정확도: %.4f" % (loaded_model.evaluate(X_test, y_test)[1]))
```